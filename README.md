# 2025_YXH

## è¯­ä¹‰æ°´å° - åŸºäºæ–‡æœ¬æç¤ºçš„å†…å®¹ç›¸å…³æ°´å° / Semantic Watermarking for Text-to-Image

This repository implements semantic watermarking for LaWa (Latent Watermarking), enabling **content-related watermarks** where each generated image has a unique watermark derived from its text prompt.

### ğŸš€ å¼€å§‹ä½¿ç”¨ / Quick Start

**æ–°ç”¨æˆ·ä»è¿™é‡Œå¼€å§‹ / New users start here:**

ğŸ‘‰ **[å¼€å§‹ä½¿ç”¨.md (5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹)](å¼€å§‹ä½¿ç”¨.md)** - æœ€å¿«çš„å…¥é—¨æ–¹å¼ / Fastest way to get started

æˆ–è€…é€‰æ‹©ä½ çš„è·¯å¾„ / Or choose your path:

1. **æƒ³ç›´æ¥å¼€å§‹è®­ç»ƒ?** â†’ [å¼€å§‹ä½¿ç”¨.md](å¼€å§‹ä½¿ç”¨.md)
2. **æƒ³ç†è§£å®Œæ•´å·¥ä½œæµç¨‹?** â†’ [è®­ç»ƒæŒ‡å—_ä¸­æ–‡.md](è®­ç»ƒæŒ‡å—_ä¸­æ–‡.md) æˆ– [TRAINING_GUIDE.md](TRAINING_GUIDE.md)
3. **æƒ³çœ‹å¯è§†åŒ–æµç¨‹å›¾?** â†’ [QUICK_REFERENCE.md](QUICK_REFERENCE.md)
4. **ä»äºŒè¿›åˆ¶LaWaè¿ç§»?** â†’ [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md)

### ğŸ“– å®Œæ•´æ–‡æ¡£ / Complete Documentation

#### ä¸­æ–‡æ–‡æ¡£ / Chinese Docs
| æ–‡æ¡£ | ç”¨é€” | æ—¶é•¿ |
|------|------|------|
| [å¼€å§‹ä½¿ç”¨.md](å¼€å§‹ä½¿ç”¨.md) | 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹ | âš¡ å¿«é€Ÿ |
| [è®­ç»ƒæŒ‡å—_ä¸­æ–‡.md](è®­ç»ƒæŒ‡å—_ä¸­æ–‡.md) | å®Œæ•´è®­ç»ƒå·¥ä½œæµç¨‹ | ğŸ“š è¯¦ç»† |

#### English Docs
| Document | Purpose | Length |
|----------|---------|--------|
| [TRAINING_GUIDE.md](TRAINING_GUIDE.md) | Complete training workflow | Comprehensive |
| [QUICK_REFERENCE.md](QUICK_REFERENCE.md) | Visual diagrams, commands | Quick scan |
| [examples/README_TRAINING.md](examples/README_TRAINING.md) | Training examples | Tutorial |
| [SEMANTIC_WATERMARKING.md](SEMANTIC_WATERMARKING.md) | Architecture & tech | Reference |
| [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) | Binary â†’ Semantic | For upgrades |

### ğŸ¯ Key Features

- **Content-Related Watermarks**: Each image gets a unique watermark based on its text prompt
- **High Security**: Cryptographic rotation matrix encryption
- **High Capacity**: 512-dimensional semantic vectors (vs 48-bit binary)
- **Flexible Training**: Works with or without image captions
- **Multiple Strategies**: Generic prompts, captions, or hybrid approach

### ğŸ’¡ How It Works (In Brief)

```
Text Prompt â†’ Semantic Vector â†’ Encryption â†’ Spatial Projection
                                                    â†“
Image â†’ VAE Encoder â†’ Latent â†’ [ Inject Watermark ] â†’ VAE Decoder â†’ Watermarked Image
                                      â†‘
                            U-Net WEmb (6 layers)
```

Each text prompt creates a unique 512-dimensional semantic vector that is:
1. **Encrypted** with a rotation matrix (cryptographic security)
2. **Projected** to spatial feature maps at each decoder layer
3. **Fused** with image features using U-Net modules
4. **Injected** as content-adaptive perturbations

### ğŸ”§ Training Workflow

**Three strategies available:**

#### 1. Generic Prompts (Recommended to Start)
```bash
python train.py --config configs/SD14_SemanticLaWa.yaml
```
- No caption data needed
- Auto-generates 100+ diverse prompts
- Wemb learns to embed any semantic vector

#### 2. Caption-Based Training
```bash
# First, add caption_file to config
python train.py --config configs/SD14_SemanticLaWa.yaml
```
- Uses real image captions
- Content-specific watermarks
- Better semantic binding

#### 3. Hybrid Approach
```bash
# Phase 1: Train with generic prompts
python train.py --max_epochs 30 --output results/phase1

# Phase 2: Fine-tune with captions
python train.py --checkpoint results/phase1/last.ckpt --max_epochs 10
```

### ğŸ“Š Expected Results

After 30-40 epochs:
- **Cosine Similarity**: >0.85 (watermark accuracy)
- **PSNR**: >40 dB (image quality)
- **SSIM**: >0.98 (structural similarity)
- Each image has unique watermark based on its prompt

### ğŸ“ Understanding Text Fusion

**Question**: *"How does text get integrated during training?"*

**Answer**: The text prompt is encoded to a semantic vector, then:

1. **Projection**: Vector â†’ Spatial feature maps (at each decoder layer)
2. **Concatenation**: Semantic features + VAE features
3. **Processing**: U-Net generates content-adaptive perturbations
4. **Injection**: Perturbations added to 6 strategic decoder layers

See [QUICK_REFERENCE.md](QUICK_REFERENCE.md) for visual diagrams.

### ğŸ§ª Testing & Examples

```bash
# Test semantic encoding
python examples/train_semantic_example.py --mode test_encoding

# View prompt diversity
python examples/train_semantic_example.py --mode demo_prompts

# Quick training demo with sample data
python examples/train_semantic_example.py --mode train --use_sample_data
```

### ğŸ“ Repository Structure

```
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ SD14_SemanticLaWa.yaml           # Training configuration
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ semanticLaWa.py                  # Main watermarking model
â”‚   â”œâ”€â”€ semanticEmbedding.py             # Text encoder + encryption
â”‚   â”œâ”€â”€ semanticDecoder.py               # Watermark extractor
â”‚   â””â”€â”€ unetWEmb.py                      # Perturbation generator
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ dataset.py                        # Dataset with prompt support
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ train_semantic_example.py        # Training examples
â”‚   â””â”€â”€ README_TRAINING.md               # Training guide
â”œâ”€â”€ QUICK_REFERENCE.md                   # Visual diagrams & commands
â”œâ”€â”€ TRAINING_GUIDE.md                    # Complete training guide
â”œâ”€â”€ SEMANTIC_WATERMARKING.md             # Technical documentation
â””â”€â”€ train.py                             # Main training script
```

### âš¡ Quick Commands

```bash
# Train with generic prompts (no captions needed)
python train.py --config configs/SD14_SemanticLaWa.yaml --batch_size 8

# Train with captions (update config first)
python train.py --config configs/SD14_SemanticLaWa.yaml --batch_size 8

# Test encoding
python examples/train_semantic_example.py --mode test_encoding

# View training guide
cat TRAINING_GUIDE.md | less
```

### ğŸ” Common Questions

**Q: Do I need captions to train?**  
A: No! You can train with auto-generated generic prompts.

**Q: How do I add captions?**  
A: Create a CSV with columns: `path,caption` and set `caption_file` in config.

**Q: Can each image have a different watermark?**  
A: Yes! Each prompt creates a unique semantic vector = unique watermark.

**Q: What if I don't have CLIP?**  
A: The system automatically uses hash-based encoding as fallback.

**Q: How long does training take?**  
A: ~30 hours for 40 epochs on a single GPU (V100/A100).

### ğŸ› ï¸ Troubleshooting

| Problem | Solution |
|---------|----------|
| Low similarity (<0.70) | Increase `semantic_loss_weight` to 3.0 |
| Watermarks too visible | Reduce `watermark_addition_weight` to 0.05 |
| Out of memory | Set `use_lightweight_wemb: True` |
| CLIP not loading | Will auto-use fallback mode |

See [QUICK_REFERENCE.md](QUICK_REFERENCE.md) for more troubleshooting.

### ğŸ“š Citation

If you use this semantic watermarking implementation, please cite:

```bibtex
@misc{semantic_lawa_2025,
    title={Semantic LaWa: Semantic Watermarking for Text-to-Image Generation},
    author={YeXiaohui},
    year={2025},
    note={Semantic upgrade of LaWa with content-related watermarks}
}
```

### ğŸ“„ License

This project builds upon the original LaWa work. Please respect the original licensing terms.