# Semantic Watermarking Training Examples

This directory contains examples and demonstrations for training the semantic watermarking model.

## Quick Start

### 1. Test Semantic Encoding

Verify that semantic encoding works correctly:

```bash
python examples/train_semantic_example.py --mode test_encoding
```

This will:
- Initialize the semantic encoder
- Encode several example prompts
- Show similarity matrix between prompts
- Test encryption/decryption

**Expected output:**
```
Semantic Encoding Test
✓ Encoder initialized (dim=512)

Similarity matrix:
         P 1   P 2   P 3   P 4  
Prompt  1 1.00  0.23  0.15  1.00
Prompt  2 0.23  1.00  0.31  0.23
Prompt  3 0.15  0.31  1.00  0.15
Prompt  4 1.00  0.23  0.15  1.00

✓ Encryption test:
  - Max difference after encrypt/decrypt: 3.04e-06
```

### 2. View Prompt Diversity

See the diverse prompts generated by the system:

```bash
python examples/train_semantic_example.py --mode demo_prompts
```

This shows 20 sample prompts from the pool of 100+ generic prompts used during training.

### 3. Train with Sample Data

Run a quick training demo with synthetic data:

```bash
python examples/train_semantic_example.py \
    --mode train \
    --use_sample_data \
    --max_epochs 2 \
    --batch_size 4
```

This will:
- Create 20 sample images
- Generate generic prompts
- Train for 2 epochs (quick demo)
- Save checkpoints to `results/semantic_training_example/`

### 4. Train with Your Data

Train with your actual dataset:

```bash
python train.py \
    --config configs/SD14_SemanticLaWa.yaml \
    --batch_size 8 \
    --max_epochs 40 \
    --learning_rate 0.00006 \
    --output results/my_training
```

## Training Strategies

### Strategy 1: Generic Prompts (Recommended to Start)

**Use case:** You don't have captions for your images.

**Config:**
```yaml
data:
  params:
    train:
      target: tools.dataset.datasetWithPrompts
      params:
        data_dir: /path/to/images
        data_list: data/train.csv
        resize: 256
        # No caption_file needed
        prompt_pool_size: 100
```

**How it works:**
- System generates 100+ diverse generic prompts
- Each image gets assigned a prompt based on its path hash
- Wemb module learns to embed any semantic vector
- Ready for T2I generation after training

**Command:**
```bash
python train.py --config configs/SD14_SemanticLaWa.yaml
```

### Strategy 2: Caption-Based Training

**Use case:** You have captions for your images.

**Prepare caption file:**
```csv
path,caption
image_001.jpg,A beautiful sunset over the ocean
image_002.jpg,A cat sitting on a window sill
image_003.jpg,A plate of delicious pasta
```

**Config:**
```yaml
data:
  params:
    train:
      target: tools.dataset.datasetWithPrompts
      params:
        data_dir: /path/to/images
        data_list: data/train.csv
        caption_file: data/train_captions.csv  # Add this
        resize: 256
```

**Command:**
```bash
python examples/train_semantic_example.py \
    --mode train_captions \
    --caption_file data/train_captions.csv
```

### Strategy 3: Hybrid (Generic → Captions)

**Phase 1:** Train with generic prompts (30 epochs)
```bash
python train.py \
    --config configs/SD14_SemanticLaWa.yaml \
    --max_epochs 30 \
    --output results/phase1_generic
```

**Phase 2:** Fine-tune with captions (10 epochs)
```bash
# First, update config to use caption_file
python train.py \
    --config configs/SD14_SemanticLaWa_captions.yaml \
    --checkpoint results/phase1_generic/checkpoints/last.ckpt \
    --max_epochs 10 \
    --output results/phase2_captions
```

## Understanding the Training Process

### What Happens During Training?

1. **Image Loading**
   - Load images from dataset
   - Apply random crops and augmentations

2. **Prompt Assignment**
   - **With captions:** Load from caption file
   - **Without captions:** Assign from prompt pool using hash(path)
   - Each image gets a consistent prompt within epoch

3. **Semantic Encoding**
   ```python
   prompt = "A beautiful landscape"
   semantic_vector = clip_encoder(prompt)  # 512-dim
   encrypted_vector = rotation_matrix @ semantic_vector
   ```

4. **Watermark Embedding (Wemb Module)**
   ```python
   # For each decoder layer:
   semantic_spatial = project_to_spatial(encrypted_vector)
   combined = concat(semantic_spatial, vae_features)
   perturbation = unet(combined)
   watermarked = vae_features + perturbation * strength
   ```

5. **Loss Optimization**
   ```python
   # Reconstruction loss (image quality)
   rec_loss = mse(watermarked, original)
   
   # Adversarial loss (realism)
   adv_loss = discriminator(watermarked)
   
   # Semantic loss (watermark accuracy)
   extracted = decoder(watermarked)
   semantic_loss = 1 - cosine_similarity(extracted, target)
   
   total = rec_loss + adv_loss + semantic_loss
   ```

### Key Metrics to Monitor

| Metric | Target | Meaning |
|--------|--------|---------|
| `train/cosine_sim` | >0.85 | Watermark embedding accuracy |
| `train/accuracy` | >0.85 | Verification pass rate |
| `train/psnr` | >40 dB | Image quality |
| `val/cosine_sim` | >0.85 | Validation accuracy |

**Training progress:**
- Epochs 1-10: cosine_sim increases from 0.3 → 0.6
- Epochs 10-20: cosine_sim increases from 0.6 → 0.8
- Epochs 20-30: cosine_sim reaches >0.85 (converged)
- Epochs 30+: Fine-tuning for robustness

## Prompt Pool Details

The system generates diverse prompts by combining templates and contexts.

**Template examples:**
- "A photo"
- "An image"
- "A detailed photo"
- "A scenic view"
- "A landscape"

**Context examples:**
- "with natural lighting"
- "with vivid colors"
- "in daylight"
- "at sunset"
- "with high contrast"

**Generated prompt examples:**
- "A photo with natural lighting"
- "A detailed photo in daylight"
- "A scenic view at sunset"
- "A landscape with vivid colors"

**Total combinations:** 100+ diverse prompts

## Text-to-Image Integration

After training, use the model for T2I generation:

```python
from models.semanticLaWa import SemanticLaWa

# Load trained model
model = SemanticLaWa.load_from_checkpoint('checkpoint.ckpt')

# Generate with specific prompt
prompt = "A white plate of food on a dining table"
metadata = {'model_version': 'v1.0', 'user_id': '12345'}

# Encode prompt
semantic_vec = model.semantic_encoder(prompt, encrypt=True, metadata=metadata)

# Generate watermarked image
watermarked = model(latent, original, semantic_vec)
```

**Key insight:** Each prompt creates a unique semantic watermark. Different prompts = different watermarks, even for the same image.

## Troubleshooting

### Issue: "CLIP model not found"

**Solution 1:** Install transformers
```bash
pip install transformers protobuf
```

**Solution 2:** Use fallback mode (hash-based encoding)
- The system automatically uses fallback if CLIP not available
- Works fine for training, though not as semantically meaningful

### Issue: "Low cosine similarity (<0.70)"

**Solutions:**
1. Train longer (40+ epochs)
2. Increase semantic loss weight:
   ```yaml
   semantic_loss_weight: 3.0  # Increase from 2.0
   ```
3. Check learning rate (try 0.00004 if too high)

### Issue: "Out of memory"

**Solutions:**
1. Use lightweight WEmb:
   ```yaml
   use_lightweight_wemb: True
   ```
2. Reduce batch size:
   ```bash
   python train.py --batch_size 4
   ```
3. Reduce image size to 256×256

### Issue: "Watermarks too visible"

**Solution:** Reduce injection strength
```yaml
watermark_addition_weight: 0.05  # Reduce from 0.1
```

## File Structure

```
examples/
├── train_semantic_example.py    # Training examples and demos
├── semantic_watermarking_demo.py # Component demonstrations
└── README_TRAINING.md            # This file

configs/
└── SD14_SemanticLaWa.yaml       # Training configuration

tools/
└── dataset.py                    # Dataset classes with prompt support

models/
├── semanticLaWa.py              # Main watermarking model
├── semanticEmbedding.py         # Text encoder + encryption
├── semanticDecoder.py           # Watermark extractor
└── unetWEmb.py                  # Perturbation generator
```

## Next Steps

1. **Read the training guide:** See `TRAINING_GUIDE.md` for detailed explanations
2. **Run the demos:** Test with `--mode test_encoding` and `--mode demo_prompts`
3. **Start training:** Begin with generic prompts, optionally add captions later
4. **Monitor metrics:** Watch cosine similarity and PSNR during training
5. **Deploy:** Use trained model for T2I generation with unique watermarks per prompt

## Additional Resources

- **Full documentation:** `SEMANTIC_WATERMARKING.md`
- **Migration guide:** `MIGRATION_GUIDE.md` (if upgrading from binary LaWa)
- **Implementation details:** `IMPLEMENTATION_SUMMARY.md`
- **Unit tests:** `tests/test_semantic_modules.py`

## Questions?

Common questions answered in `TRAINING_GUIDE.md`:
- Do I need real captions to train?
- Can I use different prompts for each image?
- How do I add captions to my dataset?
- Can I train with images first, then add text later?

All answers: **Yes!** The system is flexible and supports multiple workflows.
